{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np # 导入NumPy数学工具箱\nimport pandas as pd # 导入Pandas数据处理工具箱\ndf_heart = pd.read_csv(\"../input/heart-dataset/heart.csv\")  # 读取文件\ndf_heart.head() # 显示前5行数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_heart.target.value_counts() # 输出分类值，及各个类别数目","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt # 导入绘图工具\n# 以年龄+最大心率作为输入，查看分类结果散点图\nplt.scatter(x=df_heart.age[df_heart.target==1],\n            y=df_heart.thalach[(df_heart.target==1)], c=\"red\")\nplt.scatter(x=df_heart.age[df_heart.target==0],\n            y=df_heart.thalach[(df_heart.target==0)], marker='^')\nplt.legend([\"Disease\", \"No Disease\"]) # 显示图例\nplt.xlabel(\"Age\") # X轴-Age\nplt.ylabel(\"Heart Rate\") # Y轴-Heart Rate\nplt.show() # 显示散点图","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 把3个文本型变量转换为哑变量\na = pd.get_dummies(df_heart['cp'], prefix = \"cp\")\nb = pd.get_dummies(df_heart['thal'], prefix = \"thal\")\nc = pd.get_dummies(df_heart['slope'], prefix = \"slope\")\n# 把哑变量添加进dataframe\nframes = [df_heart, a, b, c]\ndf_heart = pd.concat(frames, axis = 1)\ndf_heart = df_heart.drop(columns = ['cp', 'thal', 'slope'])\ndf_heart.head() # 显示新的dataframe","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = df_heart.drop(['target'], axis = 1) # 构建特征集\ny = df_heart.target.values # 构建标签集\ny = y.reshape(-1,1) # -1是\t相对索引，等价于len(y)\nprint(\"张量X的形状:\", X.shape)\nprint(\"张量X的形状:\", y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler # 导入数据缩放器\nscaler = MinMaxScaler() # 选择归一化数据缩放器，MinMaxScaler\nX_train = scaler.fit_transform(X_train) # 特征归一化 训练集fit_transform\nX_test = scaler.transform(X_test) # 特征归一化 测试集transform","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 首先定义一个Sigmoid函数，输入Z，返回y'\ndef sigmoid(z):    \n    y_hat = 1/(1+ np.exp(-z))\n    return y_hat    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 然后定义损失函数\ndef loss_function(X,y,w,b):\n    y_hat = sigmoid(np.dot(X,w) + b) # Sigmoid逻辑函数 + 线性函数(wX+b)得到y'\n    loss = -(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) # 计算损失\n    cost = np.sum(loss) / X.shape[0]  # 整个数据集平均损失    \n    return cost # 返回整个数据集平均损失","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 然后构建梯度下降的函数\ndef gradient_descent(X,y,w,b,lr,iter) : #定义逻辑回归梯度下降函数\n    l_history = np.zeros(iter) # 初始化记录梯度下降过程中误差值(损失)的数组\n    w_history = np.zeros((iter,w.shape[0],w.shape[1])) # 初始化权重记录的数组\n    b_history = np.zeros(iter) # 初始化记录梯度下降过程中偏置的数组  \n    for i in range(iter): #进行机器训练的迭代\n        y_hat = sigmoid(np.dot(X,w) + b) #Sigmoid逻辑函数+线性函数(wX+b)得到y'\n        loss = -(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) # 计算损失\n        derivative_w = np.dot(X.T,((y_hat-y)))/X.shape[0]  # 给权重向量求导\n        derivative_b = np.sum(y_hat-y)/X.shape[0] # 给偏置求导\n        w = w - lr * derivative_w # 更新权重向量，lr即学习速率alpha\n        b = b - lr * derivative_b   # 更新偏置，lr即学习速率alpha\n        l_history[i] =  loss_function(X,y,w,b) # 梯度下降过程中的损失\n        print (\"轮次\", i+1 , \"当前轮训练集损失：\",l_history[i]) \n        w_history[i] = w # 梯度下降过程中权重的历史 请注意w_history和w的形状\n        b_history[i] = b # 梯度下降过程中偏置的历史\n    return l_history, w_history, b_history\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X,w,b): # 定义预测函数\n    z = np.dot(X,w) + b # 线性函数\n    y_hat = sigmoid(z) # 逻辑函数转换\n    y_pred = np.zeros((y_hat.shape[0],1)) # 初始化预测结果变量  \n    for i in range(y_hat.shape[0]):\n        if y_hat[i,0] < 0.5:\n            y_pred[i,0] = 0 # 如果预测概率小于0.5，输出分类0\n        else:\n            y_pred[i,0] = 1 # 如果预测概率大于0.5，输出分类0\n    return y_pred # 返回预测分类的结果","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def logistic_regression(X,y,w,b,lr,iter): # 定义逻辑回归模型\n    l_history,w_history,b_history = gradient_descent(X,y,w,b,lr,iter)#梯度下降\n    print(\"训练最终损失:\", l_history[-1]) # 打印最终损失\n    y_pred = predict(X,w_history[-1],b_history[-1]) # 进行预测\n    traning_acc = 100 - np.mean(np.abs(y_pred - y_train))*100 # 计算准确率\n    print(\"逻辑回归训练准确率: {:.2f}%\".format(traning_acc))  # 打印准确率\n    return l_history, w_history, b_history # 返回训练历史记录","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#初始化参数\ndimension = X.shape[1] # 这里的维度 len(X)是矩阵的行的数，维度是列的数目\nweight = np.full((dimension,1),0.1) # 权重向量，向量一般是1D，但这里实际上创建了2D张量\nbias = 0 # 偏置值\n#初始化超参数\nalpha = 1 # 学习速率\niterations = 500 # 迭代次数","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 用逻辑回归函数训练机器\nloss_history, weight_history, bias_history =  \\\n            logistic_regression(X_train,y_train,weight,bias,alpha,iterations)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = predict(X_test,weight_history[-1],bias_history[-1]) # 预测测试集\ntesting_acc = 100 - np.mean(np.abs(y_pred - y_test))*100 # 计算准确率\nprint(\"逻辑回归测试准确率: {:.2f}%\".format(testing_acc))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"逻辑回归预测分类值:\",predict(X_test,weight_history[-1],bias_history[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_history_test = np.zeros(iterations) # 初始化历史损失\nfor i in range(iterations): #求训练过程中不同参数带来的测试集损失\n    loss_history_test[i] = loss_function(X_test,y_test,\n                                         weight_history[i],bias_history[i])\nindex = np.arange(0,iterations,1)\nplt.plot(index, loss_history,c='blue',linestyle='solid')\nplt.plot(index, loss_history_test,c='red',linestyle='dashed')\nplt.legend([\"Training Loss\", \"Test Loss\"])\nplt.xlabel(\"Number of Iteration\")\nplt.ylabel(\"Cost\")\nplt.show() # 同时显示显示训练集和测试集损失曲线","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression #导入逻辑回归模型\nlr = LogisticRegression() # lr,就代表是逻辑回归模型\nlr.fit(X_train,y_train) # fit,就相当于是梯度下降\nprint(\"SK-learn逻辑回归测试准确率{:.2f}%\".format(lr.score(X_test,y_test)*100))\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}