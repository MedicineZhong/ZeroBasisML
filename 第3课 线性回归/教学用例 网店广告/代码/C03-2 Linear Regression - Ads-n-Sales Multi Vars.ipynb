{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np #导入NumPy数学工具箱\nimport pandas as pd #导入Pandas数据处理工具箱\n# 读入数据并显示前面几行的内容，这是为了确保我们的文件读入正确性\n# 示例代码是在Kaggle中数据集中读入文件，如果在本机中需要指定具体本地路径\ndf_ads = pd.read_csv('../input/advertising-simple-dataset/advertising.csv')\ndf_ads.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = np.array(df_ads) # 构建特征集，含全部特征\nX = np.delete(X, [3], axis = 1) # 删除掉标签\ny = np.array(df_ads.sales) #构建标签集，销售金额\nprint (\"张量X的阶:\",X.ndim)\nprint (\"张量X的形状:\", X.shape)\nprint (X)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = y.reshape(-1,1) #通过reshape函数把向量转换为矩阵，-1就是len(y),返回样本个数\nprint (\"张量y的形状:\", y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 将数据集进行80%（训练集）和20%（验证集）的分割\nfrom sklearn.model_selection import train_test_split\n# X_train, X_test, y_train, y_test = train_test_split(X_norm, y_norm, \n#                                    test_size=0.2, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                   test_size=0.2, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def scaler(train, test): # 定义归一化函数 ，进行数据压缩    \n    # 数据的压缩\n    min = train.min(axis=0) # 训练集最小值\n    max = train.max(axis=0) # 训练集最大值\n    gap = max - min # 最大值和最小值的差\n    train -= min # 所有数据减最小值\n    train /= gap # 所有数据除以大小值差\n    test -= min #把训练集最小值应用于测试集\n    test /= gap #把训练集大小值差应用于测试集\n    return train, test # 返回压缩后的数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def min_max_gap(train): # 计算训练集最大，最小值以及他们的差，用于后面反归一化过程\n    min = train.min(axis=0) # 训练集最小值\n    max = train.max(axis=0) # 训练集最大值\n    gap = max - min # 最大值和最小值的差\n    return min, max, gap\n    \ny_min, y_max, y_gap = min_max_gap(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_original = X_train.copy() # 保留一份训练集数据副本，用于对要预测数据归一化","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train,X_test = scaler(X_train,X_test) # 对特征归一化\ny_train,y_test = scaler(y_train,y_test) # 对标签也归一化","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x0_train = np.ones((len(X_train),1)) # 构造X_train长度的全1数组配合对Bias的点积\nX_train = np.append(x0_train, X_train, axis=1) #把X增加一系列的1\nx0_test = np.ones((len(X_test),1)) # 构造X_test长度的全1数组配合对Bias的点积\nX_test = np.append(x0_test, X_test, axis=1) #把X增加一系列的1\nprint (\"张量X的形状:\", X_train.shape)\nprint (X_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loss_function(X, y, W): # 手工定义一个MSE均方误差函数,W此时是一个向量\n    y_hat = X.dot(W.T) # 点积运算 h(x)=w_0*x_0 + w_1*x_1 + w_2*x_2 + w_3*x_3    \n    loss = y_hat.reshape((len(y_hat),1))-y # 中间过程,求出当前W和真值的差异\n    cost = np.sum(loss**2)/(2*len(X)) # 这是平方求和过程, 均方误差函数的代码实现\n    return cost # 返回当前模型的均方误差值","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gradient_descent(X, y, W, lr, iterations): # 定义梯度下降函数\n    l_history = np.zeros(iterations) # 初始化记录梯度下降过程中损失的数组\n    W_history = np.zeros((iterations,len(W))) # 初始化权重数组 \n    for iter in range(iterations): # 进行梯度下降的迭代，就是下多少级台阶\n        y_hat = X.dot(W) # 这个是向量化运行实现的假设函数   \n        loss = y_hat.reshape((len(y_hat),1))-y # 中间过程, y_hat和y真值的差\n        derivative_W = X.T.dot(loss)/(2*len(X)) #求出多项式的梯度向量\n        derivative_W = derivative_W.reshape(len(W)) \n        W = W - alpha*derivative_W # 结合下降速率更新权重\n        l_history[iter] = loss_function(X, y, W) # 损失的历史记录 \n        W_history[iter] = W # 梯度下降过程中权重的历史记录\n    return l_history, W_history # 返回梯度下降过程数据","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#首先确定参数的初始值\niterations = 300; # 迭代300次\nalpha = 0.15; #学习速率设为0.15\nweight = np.array([0.5,1,1,1]) # 权重向量，w[0] = bias\n#计算一下初始值的损失\nprint ('当前损失：',loss_function(X_train, y_train, weight))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 定义线性回归模型\ndef linear_regression(X, y, weight, alpha, iterations): \n    loss_history, weight_history = gradient_descent(X, y, \n                                                    weight, \n                                                    alpha, iterations)\n    print(\"训练最终损失:\", loss_history[-1]) # 打印最终损失\n    y_pred = X.dot(weight_history[-1]) # 进行预测\n    traning_acc = 100 - np.mean(np.abs(y_pred - y))*100 # 计算准确率\n    print(\"线性回归训练准确率: {:.2f}%\".format(traning_acc))  # 打印准确率\n    return loss_history, weight_history # 返回训练历史记录","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 调用刚才定义的线性回归模型\nloss_history, weight_history = linear_regression(X_train, y_train,\n                           weight, alpha, iterations) #训练机器","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"权重历史记录：\", weight_history)\nprint(\"损失历史记录：\", loss_history)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_plan = [250,50,50] # 要预测的X特征数据\nX_train,X_plan = scaler(X_train_original,X_plan) # 对预测数据也要归一化缩放\nX_plan = np.append([1], X_plan ) # 加一个哑特征X0 = 1\ny_plan = np.dot(weight_history[-1],X_plan) # [-1] 即模型收敛时的权重\n# 对预测结果要做反向缩放，才能得到与原始广告费用对应的预测值\ny_value = y_plan*y_gap + y_min # y_gap是当前y_train中最大值和最小值的差，y_min是最小值\nprint (\"预计商品销售额： \",y_value, \"千元\") ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}